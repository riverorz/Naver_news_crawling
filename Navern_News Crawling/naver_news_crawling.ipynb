{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#당일 총 페이지 수\n",
    "def calculator(url):\n",
    "\n",
    "    req = requests.get(url)\n",
    "    cont = req.content\n",
    "    soup = BeautifulSoup(cont, 'html.parser')\n",
    "    number = soup.select('div.title_desc.all_my')[0].text[6:-1].strip().replace(',',\"\")\n",
    "    maxpage = math.ceil(int(number)/10)\n",
    "    maxpage_repeat = (int(maxpage)-1)*10 +1 #네이버 페이지 올라가는 식 ex 1페이지 : 1 , 2페이지 11 3페이지 21 .......\n",
    "    return maxpage_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#네이버 기사 추출 함수\n",
    "def get_navernews(n_url):\n",
    "    news_content = []\n",
    "\n",
    "    get_req = requests.get(n_url)\n",
    "    get_soup = BeautifulSoup(get_req.content, 'html.parser')\n",
    "\n",
    "    title = get_soup.select('h3#articleTitle')[0].text  \n",
    "    news_content.append(title)\n",
    "\n",
    "    get_date = get_soup.select('.t11')[0].get_text()[:11]\n",
    "    news_content.append(get_date)\n",
    "\n",
    "    _text = get_soup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    n_text = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_content.append(n_text.strip())\n",
    "  \n",
    "    news_content.append(n_url)\n",
    "    \n",
    "    newspaper = get_soup.select('#footer address')[0].a.get_text()\n",
    "    news_content.append(newspaper)\n",
    "\n",
    "    return news_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 날짜 검색 입력\n",
    "query = input(\"검색어를 입력하세요 : \") \n",
    "startday = input(\"시작날짜 입력(ex:20200617) : \")\n",
    "\n",
    "startdatetime = datetime.strptime(startday , '%Y%m%d').date()\n",
    "\n",
    "endday = input(\"마지막 날짜입력(ex:20200617) : \")\n",
    "\n",
    "enddatetime = datetime.strptime(endday , '%Y%m%d').date()\n",
    "\n",
    "page = 1\n",
    "url = 'https://search.naver.com/search.naver?&where=news&query='+query+'&sm=tab_pge&sort=1&photo=0&field=0&reporter_article=&pd=3&ds='+startday+'&de='+startday+'&docid=&nso=so:dd,p:from'+startday+ 'to'+ startday+ ',a:all&mynews=0&start='+str(page)+'&refresh_start=0'\n",
    "maxpage_repeat = calculator(url)\n",
    "news_box =[]\n",
    "\n",
    "# 최종 날짜 까지 반복\n",
    "while startdatetime < enddatetime +timedelta(days=1) :\n",
    "    \n",
    "    print(\"현재 날짜\",startdatetime)\n",
    "    #총페이지 까지 반복\n",
    "    while page < maxpage_repeat +1 :\n",
    "        print(\"현재페이지\",page,\"사이트\",url)\n",
    "        req = requests.get(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    news_content = get_navernews(urls[\"href\"])\n",
    "                    year = news_content[1]\n",
    "                    newspaper= news_content[4]\n",
    "                    title=news_content[0]\n",
    "                    contents=news_content[2]\n",
    "                    link=news_content[3]\n",
    "                    news_total = [year, newspaper, title, contents, link]\n",
    "                    news_box.append(news_total)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "        url = 'https://search.naver.com/search.naver?&where=news&query='+query+'&sm=tab_pge&sort=1&photo=0&field=0&reporter_article=&pd=3&ds='+startday+'&de='+startday+'&docid=&nso=so:dd,p:from'+startday+ 'to'+ startday+ ',a:all&mynews=0&start='+str(page)+'&refresh_start=0'\n",
    "\n",
    "    startdatetime = startdatetime +timedelta(days=1)\n",
    "    startday = str(startdatetime).replace('-','')\n",
    "    page = 1\n",
    "    url = 'https://search.naver.com/search.naver?&where=news&query='+query+'&sm=tab_pge&sort=1&photo=0&field=0&reporter_article=&pd=3&ds='+startday+'&de='+startday+'&docid=&nso=so:dd,p:from'+startday+ 'to'+ startday+ ',a:all&mynews=0&start='+str(page)+'&refresh_start=0'\n",
    "    maxpage_repeat = calculator(url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() \n",
    "data = pd.DataFrame(news_box)\n",
    "data.columns = ['years','newspaper','title','contents','link']\n",
    "xlsx_outputFileName = \"./ 검색어 {}, 기간 {} - {} 실행시간 {}-{}-{} {}시 {}분.xlsx\".format(query, startday, endday,now.year, now.month, now.day, now.hour, now.minute )\n",
    "data.to_excel(xlsx_outputFileName, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
